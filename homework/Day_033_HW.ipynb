{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請觀看李宏毅教授以神奇寶貝進化 CP 值預測的範例，解說何謂機器學習與過擬合。並回答以下問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[youtube](https://www.youtube.com/watch?v=fegAeph9UaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型的泛化能力 (generalization) 是指什麼？ \n",
    "***\n",
    "- 首先在訓練模型時，會將資料分為以下幾部分:\n",
    "    1. 訓練集(training set):用來訓練模型的資料\n",
    "    2. 驗證集(validation set):用來驗證模型的資料，通常會用這裡預測的結果來評估、分析和改善模型的問題。\n",
    "    3. 測試集(testing set):測試集資料是用來模擬當模型上線運作時預測的好壞。\n",
    "    \n",
    "[ANS] 當我們訓練一個模型，會使用訓練集資料來訓練模型，而當我們訓練完成的模型要進行測試驗證時，這時候使用的資料是訓練模型時沒有看過的資料，因此預測結果的好壞就代表這個模型的泛化能力(generalization)。\n",
    "\n",
    "\n",
    "### 2. 分類問題與回歸問題分別可用的目標函數有哪些？\n",
    "***\n",
    "[ANS]\n",
    "    - 分類問題適用的目標函數:\n",
    "        1. cross entropy\n",
    "    - 回歸問題適用的目標函數:\n",
    "        1. Mean Square Error(MSE)\n",
    "        2. Mean Absolute Error(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目標函數的介紹\n",
    "- cross entropy:以下幾點是我們要探討的:\n",
    "***\n",
    "1. 為什麼cross entropy來做分類模型的目標函數，而不用MSE?\n",
    "    - 我們知道MSE的目標函數是用來**計算預測結果與實際結果的差距**\n",
    "    - 若當我們使用MSE來預測分類模型時，建立出來的模型只會單純的去fit資料，這邊兩個Model跑出來的local minmum錯誤率隨然都是0.25，但在機率輸出上Model2卻比Model1來得好。\n",
    "    - **[重點]** 這會有什麼問題? \n",
    "        - 以這樣的方式訓練模型，較難得到好的準確率，因為用錯誤率得到只知道此筆資料判別錯誤，**但模型不會知道現在的模型錯的很多還是很少**，這樣模型在學習時根本不知道最佳的模型在那的方向，也不知道要更新多少。\n",
    "\n",
    "![image info](./aa.png)\n",
    "***\n",
    "2. 什麼是cross entropy? Entropy是接收的所有訊息中所包含的資訊的平均量，所以在介紹Entropy之前要先簡單說一下什麼是訊息量。\n",
    "3. 訊息量(information gain) : 假設X是一個隨機變數，機率密度函數為p(x) = p(X)，下圖為訊息量公式。\n",
    "    - 我們用例子來說明會比較清楚: A同學是資優生，考試及格的機率是p(xA) = 0.9 ; B同學是吊車尾的學生，考試及格機率是p(xB) = 0.4，這時候\n",
    "                I(xA)=-log(0.99)=0.014 ， I(xB)=-log(0.4)=1.322       (註:這邊的log是以2為底計算)\n",
    "    - 這邊B同學的訊息量大於A同學的訊息量，為什麼呢? 因為A同學及格的機率太高了，因此他及格是很正常的，**因此信息量較少**；而B同學及格的機率不高，他的及格必然會受到較高的關注，**因此信息量較高**。 \n",
    "    - **[小結]** 這邊可以做一個小小的結論: 機率越隨機(可能一下成績高一下成績低)的情況，訊息量會比較大，機率偏大或者偏小其訊息量較小。\n",
    "![image info](./a2.png)  \n",
    "\n",
    "\n",
    "4. 回到cross-entropy的求值過程 : \n",
    "    - 在做分類問題時，每個資料都會有一組預測的機率(從上述例子來看，每個資料分為男生、女生和其他，皆有一組機率)，而cross entropy是將每一筆資料的類別的機率帶入公式，就可以得到個別的cross entropy，再將結果加總即可。\n",
    "    - ex: Model1的cross-entropy = 男生的cross entropy + 女生的cross entropy + 其他的cross entropy =2.322 + 1.322 + 3.322 = 6.966\n",
    "\n",
    "![image info](./a4.png) \n",
    "\n",
    "我們前面已經提到模型二的模型比模型一好，從cross-entropy也可以得知，cross-entropy越小，代表模型越好，這也是為什麼分類的損失函數為什麼用cross-entropy，前面有假設損失函數盡量都找越小越好的。\n",
    "![image info](./a3.png) \n",
    "\n",
    "- **本篇內容與範例引用[Tommy Huang的Medium](URL 'https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb')**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
