{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection  \n",
    "\n",
    "ANS:\n",
    "- 當我們要預測的任務時，資料是多變數(muti-feature)，以方程式來表示預測值(y)結果如下:\n",
    "            y = w1x1 + w2x2 + w3x3 + ... + b     ......(式1)\n",
    "            y是依變數, w是係數或權重, x則是變數, b是偏移量或看成常數\n",
    "      \n",
    "      而目標函數如下:\n",
    "![image info](./a5.png)\n",
    "- Lasso在目標函式中所使用的是一階懲罰式，透過Lasso正則化會使(式1)中的部分係數直接降為0，這也就是為什麼LASSO可用來作Feature selection\n",
    "- Lasso不同於Ridge，Ridge只會將係數逼近0, 而Lasso則是直接將係數降為0。因此，也就有Lasso容易造成稀疏矩陣的現象\n",
    "\n",
    "***    \n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?  \n",
    "\n",
    "ANS:\n",
    "- 當自變數存在高度共線性時，透過最小二乘法找到 y = wx+b 的參數 w 會非常大，而這會導致什麼情況呢?  \n",
    "    1. 當輸入變量(x)發生微小的變動，反映在輸出結果(y)也會非常的大，這也就是噪聲敏感的原因。\n",
    "- 如何降低w的值?\n",
    "    1. 為了限制模型參數w的大小，我們在原本的目標函數加上1個懲罰項，這個動作就叫做正則化(Regularization)\n",
    "    2. 而目標函數加上的平方項的懲罰項，就是Ridge Regression\n",
    "\n",
    "***\n",
    "#### Ridge Regression介紹:\n",
    "- Ridge Regression的懲罰項是對目標函式進行二階懲罰，故稱為L2 Penalty\n",
    "![image info](./a6.png)\n",
    "- 如上式所示，L2懲罰參數的值可以透過「tuninig parameter, λ」來控制\n",
    "    1. 當λ → 0，L2懲罰參數就跟OLS回歸一樣，目標函式只有最小化SSE\n",
    "    2. 而當λ → ∞ 時，懲罰效果最大，迫使所有係數都趨近於0\n",
    "\n",
    "\n",
    "- 下圖範例:\n",
    "![image info](./a7.png)\n",
    "    1. 由上圖可觀察，係數隨著λ由0變化到821(log(821) = 6.7)，係數逐漸被ridge正規化\n",
    "    2. 部分特徵係數會波動，直到log(λ)≃ 0才逐漸穩定開始收斂至0。這表示存在多重共線性，唯有透過log(λ)>0校正參數來限制係數，以降低模型變異和誤差。\n",
    "\n",
    "參考資料 : \n",
    "1. [Ridge Regression解說](https://www.twblogs.net/a/5b8d81432b717718833e812e)\n",
    "2. [Regularized Regression正規化迴歸](https://www.jamleecute.com/regularized-regression-ridge-lasso-elastic/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 額外補充\n",
    "#### 研究變項的關係\n",
    "***\n",
    "例子:分析男女網路的消費能力，若整年消費額達到平均值上，則標記為1，反之為0。假設當我們蒐集的資料內容有以下幾項:性別、薪水、上網時間、消費金額、消費能力。  \n",
    "\n",
    "例子:葡萄酒分類，\n",
    "\n",
    "1. 自變項(independent variable):影響別人的變項\n",
    "    - 自變項輸入模型的Input data或可稱為特徵值。\n",
    "    - 在例子中的自變項就是:性別、薪水、上網時間、消費經額。\n",
    "\n",
    "\n",
    "2. 依變項or應變項(dependent variable):被影響的變項\n",
    "    - 自變項輸入模型後對應的結果，可稱為依變項。依變項是隨著自變項的變化而改變的變項。\n",
    "    - 在例子中的依變項就是:消費能力\n",
    "\n",
    "\n",
    "3. 共變項:\n",
    "    - 在例子中，薪水、消費金額、消費能力之間存在著共變性，消費金額確實與消費能力存在正相關性，但薪水的高低同時也可能影響著消費金額與消費能力。  \n",
    "\n",
    "\n",
    "4. 多元共線性:\n",
    "    - 前言:迴歸分析(Regression analysis)可以一次檢視多個自變項對於依變項的預測效果，不過當你在作迴歸分析的時候，是否常常沒有對於自變項之間的相關性作審慎的評估，就貿然地將許多個自變項同時放到迴歸方程式裡頭?\n",
    "    - 解釋:多元共線性是指多元迴歸分析中，自變項之間有相關存在的一種現象，是一種程度的問題(degree of matters)，而不是全有或全無(all or none)的狀態。多元共線性若是達嚴重的程度時，會對多元迴歸分析造成下列的不良影響。\n",
    "    - 影響:\n",
    "        1. 膨脹最小平方法(least squares)估計參數值的變異數和共變數，使得迴歸係數的估計值變得很不精確\n",
    "        2. 膨脹迴歸係數估計值的相關係數\n",
    "        3. 膨脹預測值的變異數，但對預測能力不影響\n",
    "        4. 造成解釋迴歸係數及其信賴區間估計之困難\n",
    "        5. 造成整體模式的考驗達顯著，但各別迴歸係數之考驗不顯著的矛盾現象和解釋上之困擾\n",
    "        6. 造成迴歸係數的正負號與所期望者相反的衝突現象，這是由於自變項間之壓抑效果(suppress effect)造成的\n",
    "    - 如何找出多元共線性變項:\n",
    "        1. 一個比較簡單的診斷方法是察看自變項間的相關係數矩陣，看看該矩陣中是否有元素值（即自變項兩兩之間的相關係數值）是大於0.8以上者，若有，即表示該二變項互為多元共線性變項，並認為該迴歸分析中有嚴重的多元共線性問題存在\n",
    "        2. 另一個比較正式、客觀的診斷法，則為使用第j個自變項的「變異數膨脹因子」(variance inflation factor)作為判斷的指標，凡變異數膨脹因子指標值大於10者，即表示第j個自變項是一個多元共線性變項。\n",
    "    - 解決辦法:在一般的迴歸分析中，針對多元共線性問題，有些統計學家會建議將多元共線性變項予以刪除，不納入迴歸方程式中。但避免多元共線性問題所造成困擾的最佳解決方法，不是刪除該具有多元共線性變項，而是*使用所謂的「偏差迴歸分析」(biased regression analysis, BRA)*。\n",
    "        1. ridge regression(山脊型迴歸)，最受到學者們的重視和使用\n",
    "        2. principal component analyse(PCA)主成分分析\n",
    "        3. latent root regression(潛在根迴歸)\n",
    "        4. Baysean regression(貝氏法迴歸)\n",
    "        5. 補充:不過這些偏差迴歸分析法所獲得的迴歸係數值都是有偏差的(biased)，亦即這些迴歸係數的期望值不等於母群體的迴歸係數值，所以稱作偏差迴歸係數估計值，而本補救多元共線性問題的方法即稱作偏差迴歸分析法。\n",
    "\n",
    "***\n",
    "參考資料:  \n",
    "    1. [教育百科](https://pedia.cloud.edu.tw/Entry/Detail/?title=%E5%A4%9A%E5%85%83%E5%85%B1%E7%B7%9A%E6%80%A7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
