{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "\n",
    "\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "***\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎?  \n",
    "[ANS] :可以，但如果沒有對樹的成長作限制，演算法最後就會為每個不同特徵值創建新的分類節點，最後將所有資料作到100%正確的分類，但這也產生了Overfitting的問題。因此為了預防Overfitting，我們會採取下列兩種方式：設限及剪枝。\n",
    "\n",
    "[補充]:\n",
    "        1. 設限:\n",
    "            1. Minimum samples for a node split：資料數目不得小於多少才能再產生新節點。\n",
    "            2. Minimum samples for a terminal node (leaf)：要成為葉節點，最少需要多少資料。\n",
    "            3. Maximum depth of tree (vertical depth)：限制樹的高度最多幾層。\n",
    "            4. Maximum number of terminal nodes：限制最終葉節點的數目\n",
    "            5. Maximum features to consider for split：在分離節點時，最多考慮幾種特徵值。\n",
    "        2. Tree pruning在決策樹中的處理方式：\n",
    "            1. 針對樹設定一個適當的深度。\n",
    "            2. 從底部開始，移除所有回傳為負值（與上一層相比）的節點葉子。\n",
    "            3. 如果有一分枝回覆值為-10，但下一個分枝回覆+20，整體加總為+10，因此Tree pruning仍會保留該分枝，但一般的決策樹算法只看到分枝回覆-10便會停止而不會考慮到下一個分枝會回覆+20。\n",
    "            4. 因此，pruning可以修整決策樹到最佳決策的樹型。不過很可惜的，Python Scikit-learn尚未支援Tree pruning，必須使用額外的ML套件xgboost。\n",
    "\n",
    "參考資料:\n",
    "1. [決策樹 Decision trees_By CH.Tseng](https://chtseng.wordpress.com/2017/02/10/%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-trees/)\n",
    "\n",
    "***\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題?  \n",
    "[ANS] :也可用在回歸問題上。  \n",
    "    - 回歸樹原理概述:要以決策樹來解決回歸的核心問題在於 1.如何選擇劃分點? 2.如何決定葉節點的輸出值?\n",
    "        1. 我們用決策樹來做分類的方法，是通過計算選擇最佳的劃分點。同樣的以決策樹來做回歸，一樣得找出劃分點。\n",
    "        2. 以例子來看:假設有n個特徵，每個特徵有s個取值，當我們看完所有特徵，並得到特徵所有取值，然後對空間進行劃分，直到特徵x的取值s，使損失函數最小，這樣就找到一個劃分點。\n",
    "        \n",
    "參考資料:\n",
    "1. [Regression Tree_CSDN博主_一个拉风的名字](https://blog.csdn.net/weixin_40604987/article/details/79296427)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
